---
title: "exercise8.Rmd"
author: "Giulio Vicentini"
date: "12/6/2020"
output: html_document
---

EX 1
a)
```{r}
library(gplots)
library(mvtnorm)

metrop <- function(func, thetaInit, Nburnin, Nsamp, sampleCov, verbose,  ...) {
  
  Ntheta   <- length(thetaInit)
  thetaCur <- thetaInit
  funcCur  <- func(thetaInit, ...) # log10
  funcSamp <- matrix(data=NA, nrow=Nsamp, ncol=2+Ntheta) 
  # funcSamp will be filled and returned
  nAccept  <- 0
  acceptRate <- 0
  
  for(n in 1:(Nburnin+Nsamp)) {
    
    # Metropolis algorithm. No Hastings factor for symmetric proposal
    if(is.null(dim(sampleCov))) { # theta and sampleCov are scalars
      thetaProp <- rnorm(n=1, mean=thetaCur, sd=sqrt(sampleCov))
    } else {
      thetaProp <- rmvnorm(n=1, mean=thetaCur, sigma=sampleCov, method="eigen")
    }
    funcProp  <- func(thetaProp, ...) 
    logMR <- sum(funcProp) - sum(funcCur) # log10 of the Metropolis ratio
    #cat(n, thetaCur, funcCur, ":", thetaProp, funcProp, "\n")
    if(logMR>=0 || logMR>log10(runif(1, min=0, max=1))) {
      thetaCur   <- thetaProp
      funcCur    <- funcProp
      nAccept    <- nAccept + 1
      acceptRate <- nAccept/n
    }
    if(n>Nburnin) {
      funcSamp[n-Nburnin,1:2] <- funcCur
      funcSamp[n-Nburnin,3:(2+Ntheta)] <- thetaCur
    }
    
    # Diagnostics
    if( is.finite(verbose) && (n%%verbose==0 || n==Nburnin+Nsamp) ) {
      s1 <- noquote(formatC(n,          format="d", digits=5, flag=""))
      s2 <- noquote(formatC(Nburnin,    format="g", digits=5, flag=""))
      s3 <- noquote(formatC(Nsamp,      format="g", digits=5, flag=""))
      s4 <- noquote(formatC(acceptRate, format="f", digits=4, width=7, 
                            flag=""))
      cat(s1, "of", s2, "+", s3, s4, "\n")
    }
    
  }
  return(funcSamp)
}

# Return log10(likelihood) for parameters theta and obsdata
# dnorm(..., log=TRUE) returns log base e, so multiply by 1/ln(10) = 0.4342945
# to get log base 10
loglike.quadraticmodel <- function(theta, obsdata) {
  # convert alpha to b_1 and log10(ysig) to ysig
  theta[2] <- tan(theta[2])
  theta[4] <- 10^theta[4]
  modPred <- drop( theta[1:3] %*% t(cbind(1,obsdata$x,obsdata$x^2)) )
  # Dimensions in above mixed vector/matrix multiplication: [Ndat] = [P] %*% [P x Ndat] 
  logLike <- (1/log(10))*sum( dnorm(modPred - obsdata$y, mean=0, sd=theta[4], log=TRUE) )
  return(logLike)
}

# Return log10(unnormalized prior)
logprior.quadraticmodel <- function(theta) {
  b0Prior      <- dnorm(theta[1], mean=0, sd=10)
  alphaPrior   <- 1
  b2Prior      <- dnorm(theta[3], mean=0, sd=5)
  logysigPrior <- 1 
  logPrior <- sum( log10(b0Prior), log10(alphaPrior), log10(b2Prior), log10(logysigPrior) )
  return(logPrior)
}

# Return c(log10(prior), log10(likelihood)) (each generally unnormalized) of the quadratic model
logpost.quadraticmodel <- function(theta, obsdata) {
  logprior <- logprior.quadraticmodel(theta)
  if(is.finite(logprior)) { # only evaluate model if parameters are sensible
    return( c(logprior, loglike.quadraticmodel(theta, obsdata)) )
  } else {
    return( c(-Inf, -Inf) )
  }
}

#data
x_data <- c(2.44, 3.49, 3.78, 3.31, 3.18, 3.15, 3, 3.1, 3.6, 3.4)
y_data <- c(129, 464, 189, 562, 589, 598, 606, 562, 360, 494)
# rescaled data
x <- (x_data - mean(x_data))/sd(x_data)
y <- (y_data - mean(y_data))/sd(y_data)
obsdata <- data.frame(cbind(x,y))
plot(x, y)

#Define covariance matrix of MCMC sampling PDF: sigma=c(b_0, alpha, b_2, log10(ysig))
sampleCov <- diag(c(0.1, 0.01, 0.01, 0.01)^2)
# set thetaInit to: lsfit$coefficients, sqrt(mean(lsfit$residuals^2))
model <- lm(y ~ poly(x,2,raw=TRUE))
c0 <- model$coefficients[1]
c1 <- model$coefficients[2]
c2 <- model$coefficients[3]
thetaInit <- c(c0, c1, c2, log10(2.4))

# Run the MCMC to find postSamp, samples of the posterior PDF
set.seed(250)
allSamp <- metrop(func=logpost.quadraticmodel, thetaInit=thetaInit, Nburnin=2e4,                Nsamp=2e5, sampleCov=sampleCov, verbose=1e3, obsdata=obsdata)
# 10^(allSamp[,1]+allSamp[,2]) is the unnormalized posterior at each sample
thinSel  <- seq(from=1, to=nrow(allSamp), by=100) # thin by factor 100
postSamp <- allSamp[thinSel,]

# Plot MCMC chains and use density estimation to plot 1D posterior PDFs from these.
# Note that we don't need to do any explicit marginalization to get the 1D PDFs.
par(mfrow=c(4,2), mar=c(3.0,3.5,0.5,0.5), oma=0.5*c(1,1,1,1), mgp=c(1.8,0.6,0),          cex=0.9)
parnames <- c(expression(b[0]), expression(paste(alpha, " / rad")), expression(b[2]),               expression(paste(log, " ", sigma)))
for(j in 3:6) { # columns of postSamp
  plot(1:nrow(postSamp), postSamp[,j], type="l", xlab="iteration",                          ylab=parnames[j-2])
  
  postDen <- density(postSamp[,j], n=2^10)
  plot(postDen$x, postDen$y, type="l", lwd=1.5, yaxs="i",                                   ylim=1.05*c(0,max(postDen$y)), xlab=parnames[j-2], ylab="density")
  #abline(v=thetaTrue[j-2], lwd=1.5, lty=3)
}

# Plot all parameter samples in 2D
par(mfcol=c(3,3), mar=c(3.5,3.5,0.5,0.5), oma=c(0.1,0.1,0.1,0.5), mgp=c(2.0,0.8,0))
for(i in 1:3) {
  for(j in 2:4) {
    if(j<=i) {
      plot.new()
    } else {
      plot(postSamp[,i+2], postSamp[,j+2], xlab=parnames[i], ylab=parnames[j], pch=".")
    }
  }
}

# Find MAP and mean solutions.
# MAP = Maximum A Posteriori, i.e. peak of posterior.
# MAP is not the peak in each 1D PDF, but the peak of the 4D PDF.
# mean is easy, because samples have been drawn from the (unnormalized) posterior.
posMAP    <- which.max(postSamp[,1]+postSamp[,2]) 
thetaMAP  <- postSamp[posMAP, 3:6]

# Overplot MAP solution with original data
par(mfrow=c(1,1), mar=c(3.5,3.5,0.5,1), oma=0.1*c(1,1,1,1), mgp=c(2.0,0.8,0),            cex=1.0)
plotCI(obsdata$x, obsdata$y, xaxs="i", yaxs="i", xlab="x", ylab="y",                        uiw=10^thetaMAP[4], gap=0)
xsamp <- seq(from=-3, to=2, by=0.1)
ysamp <- cbind(1,xsamp,xsamp^2) %*% as.matrix(c(thetaMAP[1], tan(thetaMAP[2]),                      thetaMAP[3]))
lines(xsamp, drop(ysamp), lwd=2,col="blue")
legend("topright", c("Data","fit"), fill = c("black", "blue"), cex=0.95)
```

b)
```{r}
plot(xsamp*sd(x_data)+mean(x_data),ysamp*sd(y_data)+mean(y_data))
y_28 <- ysamp[which.min(abs(xsamp*sd(x_data)+mean(x_data)-2.8))]*sd(y_data)+mean(y_data)
y_26 <- ysamp[which.min(abs(xsamp*sd(x_data)+mean(x_data)-2.6))]*sd(y_data)+mean(y_data)
```

EX 2
```{r}
library(rjags)

# step function
cat("model{

    # data likelihood
    for (t in 1:N){ 
    log(mu[t]) <- b0 + b1*step(t-tau)

    D[t] ~ dpois(mu[t])
    y[t] ~ dpois(mu[t])
    }
    
    # uniform priors
    b0~dunif(-3,3)
    b1~dunif(-3,3)
    tau~dunif(1,N) 
}", file="model.bug")

# Load the model.extract
model<- "model.bug"

#data
data <- NULL
data$D <- c ( 4 , 5 , 4 , 1 , 0 , 4 , 3 , 4 , 0 , 6 ,
3 ,3 ,4 ,0 ,2 ,6 ,3 ,3 ,5 ,4 ,5 ,3 ,1 ,4 ,4 ,1 ,5 ,5 ,3 ,4 ,2 ,5 ,2 ,2 ,3 ,4 ,2 ,1 , 3, 2 ,1 ,1 ,1 ,1 ,1 ,3 ,0 ,0 ,1 ,0 ,1 ,1 ,0 ,0 ,3 ,1 ,0 ,3 ,2 ,2 ,
0 ,1 ,1 ,1 ,0 ,1 ,0 ,1 ,0 ,0 ,0 ,2 ,1 ,0 ,0 ,0 ,1 ,1 ,0 ,2 ,
2 ,3 ,1 ,1 ,2 ,1 ,1 ,1 ,1 ,2 ,4 ,2 ,0 ,0 ,0 ,1 ,4 ,0 ,0 ,0 ,
1 ,0 ,0 ,0 ,0 ,0 ,1 ,0 ,0 ,1 ,0 ,0)
data$N <- 112

plot(1851:1962, data$D)

# initialization 
parameters <- NULL
parameters$b0 <- 0
parameters$b1 <- 0
parameters$tau <- 50

#Explore the features of the chains and try to understand the effects of the burn-in, and thinning
burnIn=c(100,1000,10000)

for(i in 1:length(burnIn)){
  
  jm <- jags.model(model, data, parameters, n.adapt = burnIn[i])
  chain_burnin <- coda.samples(jm, c("b0", "b1","tau"), n.iter = 50000)
  plot(chain_burnin, col="blue")
  data_burnin <- as.data.frame( as.mcmc(chain_burnin) )

  pairs(data_burnin, 
        upper.panel=NULL,
        pch=20, 
        labels= c('b0','b1','tau'))
  
  print(summary(chain_burnin))
}

#and thinning
thin<-c(20,50,100,250)

corner_text <- function(text, location="topright"){
legend(location,legend=text, bty ="n", pch=NA) 
}

for(i in 1:length(thin))
    {
    chain_thin <- coda.samples(jm, c("b0", "b1","tau"), n.iter = 50000, thin = thin[i])
    data_thin <- as.data.frame( as.mcmc(chain_thin) )
    
    b0_chain<-as.mcmc(data_thin["b0"])
    b1_chain<-as.mcmc(data_thin["b1"])
    tau_chain<-as.mcmc(data_thin["tau"])
    
    par(mfrow=c(3,2))
    my_lags<-seq(0,30,1)
    
    y1_b0<-autocorr(b0_chain, lags=my_lags)
    plot(my_lags , y1_b0, ylim=c(0,1),
          pch=12, col="navy",
          xlab="lag", ylab="ACF", cex=1.3,main=paste("b0, thin= ",thin[i]))
    corner_text(sprintf("Sum ACF values excluding lag = 0  : %.2f", sum(y1_b0[2:length(y1_b0)])))

    y1_b1<-autocorr(b1_chain, lags=my_lags)
    plot(my_lags , y1_b1, ylim=c(0,1),
          pch=12, col="navy",
          xlab="lag", ylab="ACF", cex=1.3,main=paste("b1, thin= ",thin[i]))
    corner_text(sprintf("Sum ACF values excluding lag = 0  : %.2f", sum(y1_b1[2:length(y1_b1)])))

    y1_tau<-autocorr(tau_chain, lags=my_lags)
    plot(my_lags , y1_tau, ylim=c(0,1),
    pch=12, col="navy",
    xlab="lag", ylab="ACF", cex=1.3,main=paste("tau, thin= ",thin[i]))
    
    corner_text(sprintf("Sum ACF values excluding lag = 0  : %.2f", sum(y1_tau[2:length(y1_tau)])))
    plot(chain_thin , col="navy")
}
```

